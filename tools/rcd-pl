#!/usr/bin/env pypy
####################################################################
#
#  The librcd "project linker".
#
#  Asside from wrapping a standard object files/libraries to linked elf
#  procedure it does some post linking where it generates tables that
#  allows simple and fast lookup of debug information directly from
#  memory without requiring complex libraries.
#
#  SYNOPSIS:
#      ./rcd-pl in1.o in2.o lib.a -o out-elf
#
#  Copyright Â© 2014, Jumpstarter AB. This file is part of the librcd project.
#  This Source Code Form is subject to the terms of the Mozilla Public
#  License, v. 2.0. If a copy of the MPL was not distributed with this
#  file, You can obtain one at http://mozilla.org/MPL/2.0/.
#  See the COPYING file distributed with this project for more information.
#
####################################################################

import sys
import os
import shutil
import uuid
import subprocess
import StringIO

'''
# Overriding elftools.common.utils.struct_parse to one that saves the stream offset so we can seek back to that position later if we want to overwrite the struct.
###### <override> ######
from elftools.construct import ConstructError
import elftools.common.utils

def _custom_struct_parse(struct, stream, stream_pos=None):
    try:
        if stream_pos is not None:
            stream.seek(stream_pos)
        else:
            stream_pos = stream.tell()
        ent = struct.parse_stream(stream)
        if isinstance(ent, Container):
            ent['__tell'] = stream_pos
        return ent
    except ConstructError as e:
        raise ELFParseError(e.message)

elftools.common.utils.struct_parse = _custom_struct_parse
###### </override> ######
'''

from elftools.common.py3compat import maxint, bytes2str
from elftools.elf.elffile import ELFFile
from elftools.construct import (
    UBInt8, UBInt16, UBInt32, UBInt64, ULInt8, ULInt16, ULInt32, ULInt64,
    SBInt8, SBInt16, SBInt32, SBInt64, SLInt8, SLInt16, SLInt32, SLInt64,
    Struct, Container
)
from elftools.construct.lib.py3compat import BytesIO

import pprint

rfl_addr_index_t = Struct('rfl_addr_index_t',
    ULInt64('addr_line_map_ptr'),
    ULInt64('addr_line_map_n'),
    ULInt64('addr_file_map_ptr'),
    ULInt64('addr_file_map_n'),
    ULInt64('addr_func_map_ptr'),
    ULInt64('addr_func_map_n'),
)

rfl_addr_line_mapping_t = Struct('rfl_addr_line_mapping_t',
    ULInt32('addr'),
    ULInt32('len'),
    ULInt32('line'),
)

rfl_addr_fstr_mapping_t = Struct('rfl_addr_file_mapping_t',
    ULInt32('addr'),
    ULInt32('len'),
    ULInt16('fstr_len'),
    ULInt64('fstr_ptr'),
)

def pprint_to_file(file, data):
    f = open(file, 'w')
    f.write(pprint.pformat(data))
    f.close()

def trim_addr_map(addr_map, v_key):
    sorted_addr_map = sorted(addr_map, cmp=lambda x,y: cmp(x['addr'], y['addr']))
    prev_ent = None
    final_addr_map = []
    for ent in sorted_addr_map:
        if (ent['addr'] > 0x7fffffff):
            raise Exception('unsupported address size [' + str(ent['addr']) + ']')
        # Filter out entries with zero length.
        if ent['len'] <= 0:
            continue
        # Modify entries based on previous entry.
        if prev_ent:
            if prev_ent[v_key] == ent[v_key]:
                # Collision, merge the two entries.
                new_end_addr = ent['addr'] + ent['len']
                prev_ent['len'] = max(new_end_addr - prev_ent['addr'], prev_ent['len'])
                continue
            elif ent['addr'] < prev_end_addr:
                # Intersection, move the address forward.
                offset = prev_end_addr - ent['addr']
                ent['len'] -= offset
                if ent['len'] <= 0:
                    continue;
                ent['addr'] += offset
        final_addr_map.append(ent)
        prev_ent = ent
        prev_end_addr = ent['addr'] + ent['len']
    return final_addr_map

def strings_blob_ref(strings_blob, str):
    str = str.encode('utf-8')
    if not str in strings_blob['refs']:
        strings_blob['refs'][str] = strings_blob['addr'] + strings_blob['io'].tell()
        strings_blob['io'].write(str)
    return {'len': len(str), 'ptr': strings_blob['refs'][str]}

# Returns true if the specified frame is a generic core function that is protected
# from being callable from fibers in userspace (i.e. with mega thread stack active).
# The depth of these functions can safely be disregarded when estimating max stack usage.
def is_prot_core_librcd_frame(frame_id):
    return (frame_id == 'vm_mmap_reserve'
        or frame_id == 'vm_mmap_unreserve'
        or frame_id == 'lwt_panic')

# Returns true if the specified frame is an stack ignored core function.
def is_stack_ignored_core_frame(frame_id):
    return (
        # It's pointless to optimize frames that will only ever be called once.
           frame_id == '__assert_fail'
        or frame_id == 'abort'
        # Exceptions require lots of memory allocations to throw and is rarely
        # called in practice making the relative performance gained from
        # pre-allocation very low. The are also overwhelmingly used in leaf
        # contexts making their deep stack impact very high. We're better of
        # ignoring them in our deep stack estimation.
        or frame_id == 'lwt_throw_exception'
        or frame_id == 'lwt_throw_new_exception'
        or frame_id == 'lwt_throw_cancel_exception'
        or frame_id == 'lwt_throw_join_race_exception'
        or frame_id == 'lwt_throw_no_such_fiber_exception'
        or frame_id == '_rcd_syscall_exception')

def resolve_deep_frame_size(frame_graph, frame_id):
    if frame_id not in frame_graph:
        # Non-indexed frame, likely raw assembly function or other external symbol. (e.g. longjmp)
        # Although tecnically non-deterministic this is common and non-intresting since raw contexts use the mega stack.
        # We therefore ignore this case and say that this frame deterministically uses zero bytes.
        return 0, False
    if is_stack_ignored_core_frame(frame_id):
        # We choose to ignore some functions when estimating max stack usage
        # because the expected performance gained from pre-allocating their
        # stack is far from being worth of the required memory overhead.
        return 0, False
    frame = frame_graph[frame_id]
    deep_size = frame['deep_size']
    if deep_size is None:
        # Recursive call graph, assume no recursion and abort.
        # This indicates a non-deterministic deep stack size.
        return 0, True
    ndrm = frame['ndrm']
    if deep_size != False:
        # Size already resolved.
        return deep_size, ndrm
    # Set deep_size to None to detect recursion and go through callouts.
    frame['deep_size'] = None
    size = frame['size']
    sub_size = 0
    # Protected core frames have no depth we need to consider.
    if not is_prot_core_librcd_frame(frame_id):
        # Start recursive scan of deep size.
        for dst_frame_id in frame['callouts']:
            co_size, co_ndrm = resolve_deep_frame_size(frame_graph, dst_frame_id)
            frame['callouts'][dst_frame_id] = (co_size, co_ndrm)
            sub_size = max(sub_size, co_size)
            ndrm = (ndrm or co_ndrm)
    # Scan complete. Deep stack size estimation available.
    deep_size = size + sub_size
    frame['deep_size'] = deep_size
    frame['ndrm'] = ndrm
    return deep_size, ndrm

def main(argv):
    # Go through argv and look for -o for output and break it out.
    elf_out = None
    prev_arg = None
    del argv[0]
    for i in range(len(argv)):
        if i > 0 and argv[i - 1] == '-o':
            elf_out = argv[i]
            del argv[i]
            del argv[i - 1]
            break
    if elf_out is None:
        raise Exception("no elf output file specified")
    # Generate temporary elf in and out names.
    tmp_elf_in = "/tmp/rcd-pl-elfin-" + str(uuid.uuid1())
    tmp_elf_out = "/tmp/rcd-pl-elfout-" + str(uuid.uuid1())
    try:
        # Generate the temporary "elf in" file by linking (for real).
        link_cmd = "clang -nostdlib " + " ".join(argv) + " -o " + tmp_elf_in
        subprocess.check_output(link_cmd, shell=True)
        if not os.path.isfile(tmp_elf_in):
            raise Exception("linker generated no output");
        addr_line_map = []
        addr_file_map = []
        addr_func_map = []
        # Open the temporary elf files.
        with open(tmp_elf_in, 'rb') as f_in, open(tmp_elf_out, 'w+b') as f_out:
            # Read ELF DWARF information.
            elffile = ELFFile(f_in)
            if not elffile.has_dwarf_info():
                raise Exception("no dwarf info present in elf");
            # Find the rfl_addr_index section and get the section header offsets so we can manipulate it later.
            rfl_addr_index_shdr = elffile.get_section_by_name('rfl_addr_index')
            if (rfl_addr_index_shdr is None):
                raise Exception('missing rfl_addr_index section, is this really a librcd executable?')
            if (rfl_addr_index_shdr.header['sh_size'] != 8):
                raise Exception('invalid size of rfl_addr_index section, is this really a librcd executable?')
            # Go through all segments and find next free address where we can put the addr index segment.
            addr_index_ptr_base = 0
            for segment in elffile.iter_segments():
                end_addr = segment.header['p_vaddr'] + segment.header['p_memsz']
                addr_index_ptr_base = max(addr_index_ptr_base, end_addr)
            # Page align the address.
            addr_index_ptr_base = (addr_index_ptr_base + 0xFFF) & ~0xFFF
            # Index address -> file/line information for compilation unit.
            dwarfinfo = elffile.get_dwarf_info()
            for CU in dwarfinfo.iter_CUs():
                lineprog = dwarfinfo.line_program_for_CU(CU)
                prevstate = None
                for entry in lineprog.get_entries():
                    # We're interested in those entries where a new state is assigned.
                    if entry.state is None or entry.state.end_sequence:
                        continue
                    # Ignore erroneous ranges that span over multiple files.
                    # They are known to be caused by weak attribute and having symbol conflicts
                    # that is merged by the linker in multiple distinct object files.
                    if prevstate and (entry.state.file == prevstate.file):
                        length = entry.state.address - prevstate.address
                        line = prevstate.line
                        addr_line_map.append({'addr': prevstate.address, 'len': length, 'line': line})
                        file = lineprog['file_entry'][prevstate.file - 1].name
                        addr_file_map.append({'addr': prevstate.address, 'len': length, 'file': file})
                    prevstate = entry.state
            # Index address -> function information for compilation unit.
            for CU in dwarfinfo.iter_CUs():
                for DIE in CU.iter_DIEs():
                    if DIE.tag == 'DW_TAG_subprogram':
                        try:
                            lowpc = DIE.attributes['DW_AT_low_pc'].value
                            highpc = DIE.attributes['DW_AT_high_pc'].value
                            length = highpc - lowpc
                            func = DIE.attributes['DW_AT_name'].value
                            addr_func_map.append({'addr': lowpc, 'len': length, 'func': func})
                        except KeyError:
                            pass
            # Read and resolve frame call graph so we can scan deep frame size.
            frame_info = elffile.get_section_by_name('__librcd_frame_info')
            if frame_info is None:
                raise Exception('missing __librcd_frame_info section, is this really a librcd executable?')
            fmitosis_prefix = "__librcd_fiber_mitosis__"
            finfo_lines = StringIO.StringIO(frame_info.data())
            frame_graph = {}
            fiber_stack_map = []
            for line in finfo_lines:
                tokens = line.strip().split(';')
                finfo_id = tokens[0]
                frame_id = tokens[1]
                if frame_id not in frame_graph:
                    frame_graph[frame_id] = {'size': 0, 'deep_size': False, 'callouts': {}, 'ndrm': False}
                    if frame_id.startswith(fmitosis_prefix):
                        fiber_name = frame_id[len(fmitosis_prefix):]
                        size_section = elffile.get_section_by_name('__librcd_fse__' + fiber_name)
                        fiber_stack_map.append({'fiber_name': fiber_name, 'frame': None, 'main_frame_id': '_' + fiber_name, 'size_section': size_section})
                if finfo_id == 'frame-size':
                    frame_size = tokens[2]
                    frame_graph[frame_id]['size'] = int(frame_size)
                elif finfo_id== 'callout':
                    dst_frame_id = tokens[2]
                    frame_graph[frame_id]['callouts'][dst_frame_id] = True
                elif finfo_id == 'ndrm-callout':
                    frame_graph[frame_id]['ndrm'] = True
                else:
                    raise Exception('unknown frame info id [' + tokens[0] + ']')
            for fsm in fiber_stack_map:
                frame_id = fsm['main_frame_id']
                resolve_deep_frame_size(frame_graph, frame_id)
                fsm['frame'] = frame_graph[frame_id]
            # Sort, merge and cut away collisions in the respective maps.
            addr_line_map = trim_addr_map(addr_line_map, 'line')
            addr_file_map = trim_addr_map(addr_file_map, 'file')
            addr_func_map = trim_addr_map(addr_func_map, 'func')
            fiber_stack_map = sorted(fiber_stack_map, cmp=lambda x,y: cmp(x['fiber_name'], y['fiber_name']))
            # Create binary fast sorted lookup table structures and link them together.
            strings_blob = {'refs': {}, 'io': BytesIO(), 'addr': addr_index_ptr_base + rfl_addr_index_t.sizeof()}
            for ent in addr_file_map:
                ent['file'] = strings_blob_ref(strings_blob, ent['file'])
            for ent in addr_func_map:
                ent['func'] = strings_blob_ref(strings_blob, ent['func'])
            while (strings_blob['io'].tell() % 16) != 0:
                strings_blob['io'].write('\x00')
            addr_line_blob = BytesIO()
            for ent in addr_line_map:
                rfl_addr_line_mapping_t.build_stream(Container(
                    addr = ent['addr'],
                    len = ent['len'],
                    line = ent['line'],
                ), addr_line_blob)
            addr_file_blob = BytesIO()
            for ent in addr_file_map:
                rfl_addr_fstr_mapping_t.build_stream(Container(
                    addr = ent['addr'],
                    len = ent['len'],
                    fstr_len = ent['file']['len'],
                    fstr_ptr = ent['file']['ptr'],
                ), addr_file_blob)
            addr_func_blob = BytesIO()
            for ent in addr_func_map:
                rfl_addr_fstr_mapping_t.build_stream(Container(
                    addr = ent['addr'],
                    len = ent['len'], fstr_len = ent['func']['len'],
                    fstr_ptr = ent['func']['ptr'],
                ), addr_func_blob)
            addr_line_map_ptr = strings_blob['addr'] + strings_blob['io'].tell()
            addr_file_map_ptr = addr_line_map_ptr + addr_line_blob.tell()
            addr_func_map_ptr = addr_file_map_ptr + addr_file_blob.tell()
            addr_index_blob = BytesIO()
            rfl_addr_index_t.build_stream(Container(
                addr_line_map_ptr = addr_line_map_ptr,
                addr_line_map_n = len(addr_line_map),
                addr_file_map_ptr = addr_file_map_ptr,
                addr_file_map_n = len(addr_file_map),
                addr_func_map_ptr = addr_func_map_ptr,
                addr_func_map_n = len(addr_func_map),
            ), addr_index_blob)
            assert(rfl_addr_index_t.sizeof() == addr_index_blob.tell())
            # Build the new ELF.
            f_in.seek(0)
            shutil.copyfileobj(f_in, f_out)
            f_out.seek(0, os.SEEK_END)
            ua_addr_index_offs_base = f_out.tell()
            addr_index_offs_base = (ua_addr_index_offs_base + 0xfff) & ~0xfff
            f_out.truncate(addr_index_offs_base)
            f_out.seek(0, os.SEEK_END)
            addr_index_blob.seek(0)
            f_out.write(addr_index_blob.read())
            strings_blob['io'].seek(0)
            f_out.write(strings_blob['io'].read())
            addr_line_blob.seek(0)
            f_out.write(addr_line_blob.read())
            addr_file_blob.seek(0)
            f_out.write(addr_file_blob.read())
            addr_func_blob.seek(0)
            f_out.write(addr_func_blob.read())
            addr_index_total_size = f_out.tell() - addr_index_offs_base
            # Building a new list of program headers so we can inject the memory mapping.
            new_e_phoff = f_out.tell()
            for segment in elffile.iter_segments():
                elffile.structs.Elf_Phdr.build_stream(segment.header, f_out)
            elffile.structs.Elf_Phdr.build_stream(Container(
                p_type = 'PT_LOAD',
                p_flags = 4, # read-only, no execute
                p_offset = addr_index_offs_base,
                p_vaddr = addr_index_ptr_base,
                p_paddr = addr_index_ptr_base,
                p_filesz = addr_index_total_size,
                p_memsz = addr_index_total_size,
                p_align = 0x1000,
            ), f_out)
            f_out.seek(0)
            elffile.header['e_phoff'] = new_e_phoff
            elffile.header['e_phnum'] += 1
            elffile.structs.Elf_Ehdr.build_stream(elffile.header, f_out)
            # Write pointer to the rfl_addr_index_t.
            f_out.seek(rfl_addr_index_shdr.header['sh_offset'])
            ULInt64('rfl_addr_index').build_stream(addr_index_ptr_base, f_out)
            # Write all fiber stack size estimations.
            for fsm in fiber_stack_map:
                f_out.seek(fsm['size_section'].header['sh_offset'])
                ULInt64(fsm['fiber_name']).build_stream(fsm['frame']['deep_size'], f_out)
            # Output the final executable.
            with open(elf_out, 'w+b') as f_out_fin:
                f_out.seek(0)
                shutil.copyfileobj(f_out, f_out_fin)
                os.chmod(elf_out, 0755)
    finally:
        try:
            os.remove(tmp_elf_in)
        except OSError:
            pass
        try:
            os.remove(tmp_elf_out)
        except OSError:
            pass
    exit(0)

main(sys.argv)
